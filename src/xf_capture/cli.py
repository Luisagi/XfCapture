#!/usr/bin/env python3

import argparse
import sys
from textwrap import dedent

from xf_capture.setup import setup_workflow
from xf_capture.runner import run_pipeline


def build_parser() -> argparse.ArgumentParser:
    """
    Build the main argument parser for xf_capture.
    """

    parser = argparse.ArgumentParser(
        prog="xf_capture",
        description=dedent(
            """
            This tool is a Snakemake-based workflow for processing 
            paired-end sequencing data generated by Xf-TSCE experiments.
            """
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=dedent(
            """
            Typical usage:

              xf_capture setup --dir xf_workflow
              xf_capture run -i reads/ -o results/

            For help on a specific command:

              xf_capture <command> --help
            """
        ),
    )
    # Add version flag
    parser.add_argument(
        "-v", "--version",
        action="store_true",
        help="Show xf_capture version and exit"
    )

    subparsers = parser.add_subparsers(
        title="Available commands",
        dest="command",
        metavar="<command>",
    )

    # ------------------------------------------------------------------
    # setup command
    # ------------------------------------------------------------------
    setup_parser = subparsers.add_parser(
        "setup",
        help="Prepare workflow resources (references, databases, configs)",
        description=dedent(
            """
            Prepare the XfCapture workflow environment.

            This command initializes the workflow directory, extracts
            reference sequences, downloads required databases (i.e. Kraken2),
            and stores user-specific configuration.
            """
        ),
    )

    setup_parser.add_argument(
        "--dir",
        required=True,
        metavar="",
        help="Directory where the workflow environment will be created",
    )

    setup_parser.add_argument(
        "--k2-db",
        choices=["8GB", "16GB"],
        default="8GB",
        metavar="",
        help="Kraken2 database size to download: 8GB or 16GB (default: 8GB)",
    )

    setup_parser.set_defaults(func=run_setup)

    # ------------------------------------------------------------------
    # run command
    # ------------------------------------------------------------------
    run_parser = subparsers.add_parser(
        "run",
        help="Run the XfCapture Snakemake pipeline",
        description=dedent(
            """
            Execute the XfCapture pipeline on paired-end sequencing data.

            This command generates a configuration file and launches
            Snakemake using the packaged workflow.
            """
        ),
    )

    io_group = run_parser.add_argument_group("Input / Output options")
    io_group.add_argument(
        "-i",
        "--input-dir",
        required=True,
        metavar="",
        help="Directory containing paired-end FASTQ files",
    )
    io_group.add_argument(
        "-o",
        "--output-dir",
        required=True,
        metavar="",
        help="Directory where results will be written",
    )

    workflow_group = run_parser.add_argument_group("Workflow configuration")
    workflow_group.add_argument(
        "--workflow-dir",
        metavar="",
        help="Path to an existing workflow directory (optional)",
    )
    workflow_group.add_argument(
        "--kraken-db",
        metavar="",
        help="Path to a Kraken2 database (optional)",
    )

    resources_group = run_parser.add_argument_group("Resource allocation")
    resources_group.add_argument(
        "--cores",
        type=int,
        default=16,
        metavar="",
        help="Total number of CPU cores to use (default: 16)",
    )
    resources_group.add_argument(
        "--kraken-jobs",
        type=int,
        default=1,
        metavar="",
        help="Number of parallel Kraken2 jobs (default: 1)",
    ) 
    resources_group.add_argument(
        "--alignment-jobs",
        type=int,
        default=4,
        metavar="",
        help="Number of parallel alignment jobs (default: 4)",
    )
    resources_group.add_argument(
        "--iqtree-jobs",
        type=int,
        default=2,
        metavar="",
        help="Number of parallel IQ-TREE jobs (default: 2)",
    )
    resources_group.add_argument(
        "--iqtree-threads",
        type=int,
        default=8,
        metavar="",
        help="Number of threads per IQ-TREE job (default: 8)",
    )
    resources_group.add_argument(
        "--kraken-threads",
        type=int,
        default=8,
        metavar="",
        help="Number of threads per Kraken2 job (default: 8)",
    )
    run_parser.add_argument(
        "--no-auto",
        action="store_true",
        help="Disable automatic continuation to phylogenetic analysis (requires confirmation)",
    )

    run_parser.add_argument(
        "--k2-mapping-memory",
        action="store_true",
        default=False,
        help="Enable Kraken2 memory mapping mode (avoids loading entire database into RAM, default: False)",
    )

    run_parser.set_defaults(func=run_run)

    return parser


def run_setup(args: argparse.Namespace) -> None:
    """Dispatch setup command."""
    setup_workflow(args.dir, k2_db=args.k2_db)


def run_run(args: argparse.Namespace, extra_args: list = None) -> None:
    """Dispatch run command."""
    run_pipeline(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        workflow_dir=args.workflow_dir,
        kraken_db=args.kraken_db,
        cores=args.cores,
        kraken_jobs=args.kraken_jobs,
        alignment_jobs=args.alignment_jobs,
        iqtree_jobs=args.iqtree_jobs,
        kraken_threads=args.kraken_threads,
        iqtree_threads=args.iqtree_threads,
        auto=not args.no_auto,
        k2_mapping_memory=args.k2_mapping_memory,
        extra_args=extra_args if extra_args else None,
    )


def main() -> None:
    try:
        from importlib.metadata import version
    except ImportError:
        from importlib_metadata import version

    parser = build_parser()
    args, extra_args = parser.parse_known_args()

    # Handle version flag
    if getattr(args, "version", False):
        try:
            pkg_version = version("xf_capture")
        except Exception:
            pkg_version = "unknown"
        print(f"version v.{pkg_version}")
        sys.exit(0)

    if not hasattr(args, "func"):
        parser.print_help()
        sys.exit(0)

    # Pass extra arguments if running the pipeline
    if args.command == "run":
        run_run(args, extra_args)
    else:
        args.func(args)
