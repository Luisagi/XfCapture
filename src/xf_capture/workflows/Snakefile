
# ===============================================================================
# XfCapture Pipeline - Snakemake Workflow
# ===============================================================================
#
# PHASE 1 (rule all):
#   - Quality control & read trimming
#   - Taxonomic classification
#   - Probe-based sequence reconstruction
#   - MLST typing
#
# PHASE 2 (rule phylogeny_phase):
#   - Gene extraction from references
#   - Multiple sequence alignment
#   - Phylogenetic reconstruction
#   - Tree visualization
# ===============================================================================



# -------------------------------------------------------------------------------
# Imports and Configuration
# -------------------------------------------------------------------------------

import os
import sys
import re
from pathlib import Path


# Add utils directory to Python path
WORKFLOW_DIR = Path(workflow.basedir).resolve()
UTILS_DIR = WORKFLOW_DIR.parent / "utils"
sys.path.insert(0, str(UTILS_DIR.parent))

from xf_capture.utils.auxiliar_functions import (
    get_samples,
    get_r1,
    get_r2,
    get_fasta_files,
    get_gene_names,
    get_alignment_outputs,
    get_phylo_outputs,
    print_fasta_headers,
    get_successful_samples
)


# Load configuration file
configfile: 'config.yaml'



# -------------------------------------------------------------------------------
# Global Variables
# -------------------------------------------------------------------------------


# Working directory and input paths
workdir: config['directories']['output_dir']
INPUT_DIR = config['directories']['fastq_dir']
XF_REFS = config['references']['xf_genomes']


# Sample naming constraints
NOT_ALLOWED_CHARS = set("_*#@%^/! ?&:;|<>")
VALID_EXTENSIONS = (".fastq", ".fq", ".fastq.gz", ".fq.gz")



# -------------------------------------------------------------------------------
# Sample Discovery
# -------------------------------------------------------------------------------


# Get paired-end samples from input directory
SAMPLES_DICT = get_samples(INPUT_DIR, VALID_EXTENSIONS, NOT_ALLOWED_CHARS)
SAMPLES = SAMPLES_DICT.keys()


# Display discovered samples
print(f"\nTotal samples found: {len(SAMPLES)}")
for sample, (r1, r2) in SAMPLES_DICT.items():
    print(f"  - {sample}:\n      R1 → {r1}\n      R2 → {r2}")


# Get reference genomes for phylogenetic analysis
XF_DICT = get_fasta_files(XF_REFS)
XF_GENOMES = XF_DICT.keys()



# -------------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------------

def get_r1_wrapper(wildcards):
    """Wrapper function to retrieve R1 file path for a given sample."""
    return get_r1(wildcards, SAMPLES_DICT)


def get_r2_wrapper(wildcards):
    """Wrapper function to retrieve R2 file path for a given sample."""
    return get_r2(wildcards, SAMPLES_DICT)



# ===============================================================================
# PHASE 1: Main Pipeline (QC → Taxonomy → Reconstruction → MLST)
# ===============================================================================

rule all:
    """
    Phase 1 target rule.
    Executes the main pipeline up to MLST typing and reconstruction checkpoints.
    """
    input:
        # Quality control outputs
        qc_report = "01.pre-processing/qc_report.html",

        # Taxonomic classification outputs
        rcf_report = "02.tax-classification/recentrifuge_report.html",
        xf_reads_R1 = expand("02.tax-classification/xf_taxid_2370/{sample}_R1.fastq.gz", sample=SAMPLES),
        xf_reads_R2 = expand("02.tax-classification/xf_taxid_2370/{sample}_R2.fastq.gz", sample=SAMPLES),

        # Probe reconstruction outputs
        bam_stats = expand("03.probes_reconstruction/stats/{sample}_mapped_stats.tsv", sample=SAMPLES),
        consensus_filtered = expand("03.probes_reconstruction/consensus/{sample}_cns_filtered.fasta", sample=SAMPLES),
        recon_stats = expand("03.probes_reconstruction/stats/{sample}_recon_stats.tsv", sample=SAMPLES),
        summary_stats = "03.probes_reconstruction/reconstruction_summary.xlsx",
        unmapped_R1 = expand("03.probes_reconstruction/unmapped/{sample}_unmapped_R1.fastq.gz", sample=SAMPLES),
        unmapped_R2 = expand("03.probes_reconstruction/unmapped/{sample}_unmapped_R2.fastq.gz", sample=SAMPLES),

        # MLST and checkpoint outputs
        mlst = "04.mlst-typing/mlst_summary.csv",
        checkpoint = expand("03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt", sample=SAMPLES)


# ==============================================================================
# SECTION 1: Quality Control and Read Trimming
# ==============================================================================

rule fastp:
    """
    Quality control and adapter trimming using fastp.

    Processes paired-end reads to:
      - Detect and remove adapters
      - Filter low-quality reads (Q < 25)
      - Remove short reads (< 50 bp)
      - Trim low-quality bases from ends
    """
    input:
        r1 = get_r1_wrapper,
        r2 = get_r2_wrapper
    output:
        r1 = temp("01.pre-processing/{sample}_trim_R1.fastq"),
        r2 = temp("01.pre-processing/{sample}_trim_R2.fastq"),
        json = temp("01.pre-processing/{sample}_fastp.json"),
        html = temp("01.pre-processing/{sample}_fastp.html")
    params:
        min_length = 50,
        min_quality = 25
    resources:
        threads = 4
    conda:
        "envs/qc.yaml"
    log:
        "logs/fastp_{sample}.log"
    message:
        "--- [QC] Fastp: Filtering and trimming reads for {wildcards.sample} ---"
    shell:
        """
        fastp \
            --detect_adapter_for_pe \
            --length_required {params.min_length} \
            --qualified_quality_phred {params.min_quality} \
            --n_base_limit 0 \
            --cut_front \
            --cut_right \
            --thread {resources.threads} \
            -i {input.r1} \
            -I {input.r2} \
            -o {output.r1} \
            -O {output.r2} \
            -j {output.json} \
            -h {output.html} \
            > {log} 2>&1
        """


rule multiqc:
    """
    Aggregate quality control reports using MultiQC.

    Combines all fastp JSON reports into a single HTML summary.
    """
    input:
        expand("01.pre-processing/{sample}_fastp.json", sample=SAMPLES)
    output:
        report = "01.pre-processing/qc_report.html"
    conda:
        "envs/qc.yaml"
    log:
        "logs/multiqc.log"
    message:
        "--- [QC] MultiQC: Aggregating quality control reports ---"
    shell:
        """
        multiqc 01.pre-processing/ \
            --filename qc_report.html \
            --outdir 01.pre-processing/ \
            --force \
            > {log} 2>&1
        """


# ==============================================================================
# SECTION 2: Taxonomic Classification
# ==============================================================================

rule kraken2:
    """
    Taxonomic classification of reads using Kraken2.

    Classifies trimmed reads against a pre-built Kraken2 database to identify
    taxonomic composition and filter Xylella fastidiosa reads.
    """
    input:
        r1 = "01.pre-processing/{sample}_trim_R1.fastq",
        r2 = "01.pre-processing/{sample}_trim_R2.fastq"
    output:
        k2_krk = temp("02.tax-classification/{sample}.krk"),
        k2_rep = temp("02.tax-classification/{sample}.txt")
    params:
        db = config['kraken2']['database'],
        memory_mapping = "--memory-mapping" if config['kraken2'].get('memory_mapping', False) else ""
    resources:
        threads = config['threads'].get('kraken2', 8),
        kraken_jobs = 1
    conda:
        "envs/taxa-classification.yaml"
    log:
        "logs/kraken2_{sample}.log"
    message:
        "--- [TAXONOMY] Kraken2: Classifying reads for {wildcards.sample} ---"
    shell:
        """
        kraken2 \
            --db {params.db} \
            {params.memory_mapping} \
            --threads {resources.threads} \
            --output {output.k2_krk} \
            --report {output.k2_rep} \
            --paired {input.r1} {input.r2} \
            > {log} 2>&1
        """


rule download_taxdump:
    """
    Download NCBI taxonomy database files for Recentrifuge.

    Downloads and extracts the taxdump files required for generating
    taxonomic visualization reports.
    """
    output:
        tax_dump = temp(directory("02.tax-classification/tax_dump"))
    conda:
        "envs/taxa-classification.yaml"
    priority: 
        10
    log:
        "logs/download_taxdump.log"
    message:
        "--- [TAXONOMY] Downloading NCBI taxonomy database ---"
    shell:
        """
        # Remove any corrupted taxdmp.zip from previous failed attempts
        if [ -f taxdmp.zip ]; then
            rm -f taxdmp.zip
        fi

        # Download and extract taxonomy database
        retaxdump --nodespath {output.tax_dump} > {log} 2>&1
        
        # Clean up downloaded zip file
        rm -f taxdmp.zip
        """


rule recentrifuge:
    """
    Generate interactive taxonomic classification report using Recentrifuge.

    Creates an HTML report visualizing the taxonomic composition of all samples
    based on Kraken2 classification results.
    """
    input:
        k2_rep = expand("02.tax-classification/{sample}.krk", sample=SAMPLES),
        tax_dump = "02.tax-classification/tax_dump"
    output:
        rcf_rep = "02.tax-classification/recentrifuge_report.html"
    params:
        summary = "AVOID"
    conda:
        "envs/taxa-classification.yaml"
    log:
        "logs/recentrifuge.log"
    message:
        "--- [TAXONOMY] Recentrifuge: Generating taxonomic report ---"
    shell:
        """
        rcf \
            --nodespath {input.tax_dump} \
            --kraken 02.tax-classification/ \
            --outprefix {output.rcf_rep} \
            --summary {params.summary} \
            --avoidcross \
            > {log} 2>&1
        """


rule extract_xf_reads:
    """
    Extract Xylella fastidiosa reads from Kraken2 output.

    Uses KrakenTools to filter reads classified as Xylella fastidiosa
    (taxID 2370) including all child taxa for downstream analysis.
    """
    input:
        r1 = "01.pre-processing/{sample}_trim_R1.fastq",
        r2 = "01.pre-processing/{sample}_trim_R2.fastq",
        k2_krk = "02.tax-classification/{sample}.krk",
        k2_rep = "02.tax-classification/{sample}.txt"
    output:
        r1 = "02.tax-classification/xf_taxid_2370/{sample}_R1.fastq.gz",
        r2 = "02.tax-classification/xf_taxid_2370/{sample}_R2.fastq.gz"
    params:
        taxid = 2370,  # Xylella fastidiosa NCBI taxonomy ID
        tmp_r1 = "02.tax-classification/xf_taxid_2370/{sample}_R1.fastq",
        tmp_r2 = "02.tax-classification/xf_taxid_2370/{sample}_R2.fastq"
    conda:
        "envs/taxa-classification.yaml"
    message:
        "--- [TAXONOMY] Extracting Xylella fastidiosa reads for {wildcards.sample} ---"
    shell:
        """
        # Extract reads matching Xf taxonomy
        extract_kraken_reads.py \
            -k {input.k2_krk} \
            -r {input.k2_rep} \
            -1 {input.r1} \
            -2 {input.r2} \
            -t {params.taxid} \
            -o {params.tmp_r1} \
            -o2 {params.tmp_r2} \
            --fastq-output \
            --include-children \
            > /dev/null 2>&1

        # Compress output files
        pigz -c {params.tmp_r1} > {output.r1}
        pigz -c {params.tmp_r2} > {output.r2}

        # Clean up temporary files
        rm -f {params.tmp_r1} {params.tmp_r2}
        """


# ==============================================================================
# SECTION 3: Probe-Based Sequence Reconstruction
# ==============================================================================

rule build_probes_index:
    """
    Build BWA index for probe sequences.

    Creates the index files required for mapping reads to the probe reference.
    """
    input:
        fasta = config['references']['probes']
    output:
        index1 = temp("03.probes_reconstruction/probes.amb"),
        index2 = temp("03.probes_reconstruction/probes.ann"),
        index3 = temp("03.probes_reconstruction/probes.bwt"),
        index4 = temp("03.probes_reconstruction/probes.pac"),
        index5 = temp("03.probes_reconstruction/probes.sa")
    params:
        basename = "03.probes_reconstruction/probes"
    conda:
        "envs/mapping.yaml"
    log:
        "logs/build_bwa_index_probes.log"
    message:
        "--- [MAPPING] BWA: Building probe index ---"
    shell:
        """
        bwa index -p {params.basename} {input.fasta} > {log} 2>&1
        """


rule bwa_mapping:
    """
    Map Xylella fastidiosa reads to probe sequences using BWA-MEM.

    Aligns the extracted Xf reads to the probe reference and produces
    a sorted BAM file for variant calling.
    """
    input:
        index1 = "03.probes_reconstruction/probes.amb",
        index2 = "03.probes_reconstruction/probes.ann",
        index3 = "03.probes_reconstruction/probes.bwt",
        index4 = "03.probes_reconstruction/probes.pac",
        index5 = "03.probes_reconstruction/probes.sa",
        r1 = "02.tax-classification/xf_taxid_2370/{sample}_R1.fastq.gz",
        r2 = "02.tax-classification/xf_taxid_2370/{sample}_R2.fastq.gz"
    output:
        bam_sorted = temp("03.probes_reconstruction/{sample}_mapped.sorted.bam")
    params:
        db = "03.probes_reconstruction/probes"
    resources:
        threads = 4
    conda:
        "envs/mapping.yaml"
    log:
        "logs/bwa_mapping_{sample}.log"
    message:
        "--- [MAPPING] BWA: Mapping reads for {wildcards.sample} ---"
    shell:
        """
        bwa mem -t {resources.threads} {params.db} {input.r1} {input.r2} 2>> {log} | \
        samtools sort -@ {resources.threads} -o {output.bam_sorted} - 2>> {log}
        """


rule bam_stats:
    """
    Generate mapping statistics from BAM file.

    Creates a TSV file with per-reference sequence mapping statistics
    including read counts and coverage information.
    """
    input:
        bam = "03.probes_reconstruction/{sample}_mapped.sorted.bam"
    output:
        stats = "03.probes_reconstruction/stats/{sample}_mapped_stats.tsv",
        bai = temp("03.probes_reconstruction/{sample}_mapped.sorted.bam.bai")
    conda:
        "envs/mapping.yaml"
    log:
        "logs/bam_stats_{sample}.log"
    message:
        "--- [MAPPING] Samtools: Generating BAM statistics for {wildcards.sample} ---"
    shell:
        """
        samtools index {input.bam}
        (echo -e "#Refsequence\tSequence_length\tMapped_read_segments\tUnmapped_read_segments"; samtools idxstats {input.bam}) > {output.stats}
        """


rule extract_unmapped_reads:
    """
    Extract unmapped reads from BAM file.

    Retrieves paired reads that did not map to probe sequences for
    potential further analysis or quality assessment.
    """
    input:
        bam = "03.probes_reconstruction/{sample}_mapped.sorted.bam"
    output:
        unmapped_R1 = "03.probes_reconstruction/unmapped/{sample}_unmapped_R1.fastq.gz",
        unmapped_R2 = "03.probes_reconstruction/unmapped/{sample}_unmapped_R2.fastq.gz"
    resources:
        threads = 4
    conda:
        "envs/mapping.yaml"
    log:
        "logs/extract_unmapped_reads_{sample}.log"
    message:
        "--- [MAPPING] Extracting unmapped reads for {wildcards.sample} ---"
    shell:
        """
        samtools sort -n {input.bam} 2>> {log} | \
        samtools fastq -@ {resources.threads} -f 4 -F 264 -c 1 \
            -1 {output.unmapped_R1} \
            -2 {output.unmapped_R2} \
            2>> {log}
        """


rule bcftools_consensus:
    """
    Call variants and generate consensus sequences using BCFtools.

    Performs variant calling on mapped reads and generates:
      - VCF file with called variants
      - BED file masking low-coverage regions (< 3x)
      - Consensus FASTA with Ns for low-coverage positions
      - Filtered consensus with only complete sequences (no Ns)
    """
    input:
        bam = "03.probes_reconstruction/{sample}_mapped.sorted.bam"
    output:
        vcf = temp("03.probes_reconstruction/consensus/{sample}.vcf.gz"),
        csi = temp("03.probes_reconstruction/consensus/{sample}.vcf.gz.csi"),
        bed = temp("03.probes_reconstruction/consensus/{sample}_lowcov.bed"),
        consensus = "03.probes_reconstruction/consensus/{sample}_cns.fasta",
        consensus_filtered = "03.probes_reconstruction/consensus/{sample}_cns_filtered.fasta"
    params:
        fasta = config['references']['probes'],
        ploidy = 1,
        min_depth = 3
    conda:
        "envs/mapping.yaml"
    log:
        "logs/bcftools_consensus_{sample}.log"
    message:
        "--- [CONSENSUS] BCFtools: Generating consensus for {wildcards.sample} ---"
    shell:
        """
        # Generate VCF from pileup
        bcftools mpileup -Ou -f {params.fasta} {input.bam} 2>> {log} | \
        bcftools call -c --ploidy {params.ploidy} -Oz -o {output.vcf} 2>> {log}
        bcftools index {output.vcf}

        # Create BED mask for low-coverage regions
        samtools depth -aa {input.bam} | \
            awk -v min_depth={params.min_depth} '$3 < min_depth {{print $1"\t"$2-1"\t"$2}}' \
            > {output.bed} 2>> {log}

        # Generate consensus with Ns for masked regions
        bcftools consensus -f {params.fasta} -m {output.bed} {output.vcf} \
            > {output.consensus} 2>> {log}

        # Filter out sequences containing Ns (incomplete reconstruction)
        seqkit grep -s -v -r -p "N" {output.consensus} > {output.consensus_filtered}

        # Extract plasmid sequences only if present
        if grep -q "^>plasmid__" {output.consensus_filtered}; then
            seqkit grep -r -p "^plasmid__" {output.consensus_filtered} \
            >> "03.probes_reconstruction/consensus/{sample}_plasmids.fasta"
        fi
        """


rule reconstruction_stats:
    """
    Calculate reconstruction statistics for consensus sequences.

    Computes per-sequence statistics including:
      - Sequence length
      - Number of reconstructed bases
      - Reconstruction percentage
    """
    input:
        consensus = "03.probes_reconstruction/consensus/{sample}_cns.fasta"
    output:
        recon_stats = "03.probes_reconstruction/stats/{sample}_recon_stats.tsv"
    log:
        "logs/reconstruction_stats_{sample}.log"
    message:
        "--- [STATS] Calculating reconstruction statistics for {wildcards.sample} ---"
    shell:
        """
        echo -e "#Refsequence\tSequence_length\tReconstructed_bases\tReconstruction_percent" > {output.recon_stats}

        awk '
            BEGIN {{ OFS = "\\t" }}
            /^>/ {{
                if (seq != "") {{
                    len = length(seq)
                    n = gsub(/[Nn]/, "", seq)
                    recon = ((len - n) / len) * 100
                    print substr(header, 2), len, len - n, recon >> "{output.recon_stats}"

                    if (recon < 100) {{
                        print "WARNING: Low reconstruction for " substr(header, 2) ": " recon "%" >> "{log}"
                    }}
                    seq = ""
                }}
                header = $0
                next
            }}
            {{ seq = seq $0 }}
            END {{
                if (seq != "") {{
                    len = length(seq)
                    n = gsub(/[Nn]/, "", seq)
                    recon = ((len - n) / len) * 100
                    print substr(header, 2), len, len - n, recon >> "{output.recon_stats}"
                }}
            }}
        ' {input.consensus} > {log} 2>&1
        """


rule summary_reconstruction:
    """
    Generate summary Excel report for all reconstruction statistics.

    Aggregates per-sample statistics into a comprehensive summary file
    using an R script for data processing and formatting.
    """
    input:
        recon_stats = expand("03.probes_reconstruction/stats/{sample}_recon_stats.tsv", sample=SAMPLES),
        mapped_stats = expand("03.probes_reconstruction/stats/{sample}_mapped_stats.tsv", sample=SAMPLES)
    output:
        summary_out = "03.probes_reconstruction/reconstruction_summary.xlsx"
    params:
        stats_dir = "03.probes_reconstruction/stats/",
        summary_script = str(UTILS_DIR / "reconstruction_summary.R")
    conda:
        "envs/r_tools.yaml"
    log:
        "logs/summary_reconstruction.log"
    message:
        "--- [STATS] Generating reconstruction summary report ---"
    shell:
        """
        unset R_HOME
        export R_LIBS_USER=""
        export R_LIBS_SITE=""
        Rscript {params.summary_script} {params.stats_dir} {output.summary_out} >> {log} 2>&1
        """


rule verify_reconstruction:
    """
    Checkpoint: Verify successful probe reconstruction for each sample.

    Creates a checkpoint file indicating whether the sample has
    successfully reconstructed sequences for downstream phylogenetic analysis.
    """
    input:
        consensus_filtered = "03.probes_reconstruction/consensus/{sample}_cns_filtered.fasta"
    output:
        check_file = "03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt"
    log:
        "logs/verify_reconstruction_{sample}.log"
    message:
        "--- [CHECKPOINT] Verifying reconstruction for {wildcards.sample} ---"
    shell:
        """
        mkdir -p $(dirname {output.check_file})

        if [ -s {input.consensus_filtered} ] && grep -q "^>" {input.consensus_filtered}; then
            echo "SUCCESS: Sample {wildcards.sample} has reconstructed sequences" > {output.check_file}
            echo "Reconstruction successful for sample {wildcards.sample}" >> {log}

            seq_count=$(grep -c "^>" {input.consensus_filtered})
            echo "Total sequences reconstructed: $seq_count" >> {output.check_file}
            echo "Total sequences reconstructed: $seq_count" >> {log}
        else
            echo "FAILED: Sample {wildcards.sample} has no reconstructed sequences" > {output.check_file}
            echo "No sequences found in {input.consensus_filtered}" >> {log}
            echo "This sample will be excluded from downstream analyses" >> {output.check_file}
            echo "Sample {wildcards.sample} excluded from downstream analysis" >> {log}
        fi
        """


# ==============================================================================
# SECTION 4: MLST Typing
# ==============================================================================

rule mlst_typing:
    """
    MLST typing of Xylella fastidiosa strains.

    Uses the MLST tool to identify sequence types based on the
    reconstructed consensus sequences from all samples.
    """
    input:
        consensus = expand("03.probes_reconstruction/consensus/{sample}_cns.fasta", sample=SAMPLES)
    output:
        mlst = "04.mlst-typing/mlst_summary.csv"
    resources:
        threads = 4
    conda:
        "envs/mlst.yaml"
    log:
        "logs/mlst_typing.log"
    message:
        "--- [MLST] Typing Xylella fastidiosa strains ---"
    shell:
        """
        mlst --threads {resources.threads} --csv --nopath {input.consensus} >> {output.mlst} 2>> {log}
        """


# ==============================================================================
# PHASE 2: Phylogenetic Analysis (for successful samples only)
# ==============================================================================

rule extract_genes_from_references:
    """
    Extract target genes from all Xylella fastidiosa reference genomes.

    Uses BLAST to identify and extract gene sequences from reference genomes
    based on the probe sequences. Creates per-genome FASTA files for alignment.
    """
    input:
        xf_genomes = [XF_DICT[fasta][0] for fasta in XF_GENOMES]
    output:
        refs = expand("05.phylogenetic_trees/refs/{fasta}/{fasta}.bed.fasta", fasta=XF_GENOMES),
        ref_dir = temp(directory("05.phylogenetic_trees/refs/"))
    params:
        query = config['references']['probes'],
        basenames = list(XF_GENOMES)
    conda:
        "envs/extract_genes.yaml"
    log:
        "logs/extract_genes_from_references.log"
    message:
        "--- [PHYLO] Extracting genes from reference genomes ---"
    shell:
        """
        mkdir -p {output.ref_dir}

        # Create arrays for parallel processing
        xf_genomes=({input.xf_genomes})
        basenames=({params.basenames})

        for i in "${{!xf_genomes[@]}}"; do
            genome="${{xf_genomes[$i]}}"
            basename="${{basenames[$i]}}"

            echo "Processing genome: $basename" >> {log}
            mkdir -p "{output.ref_dir}/$basename"

            db_name="{output.ref_dir}/$basename/$basename"
            blastn_out="{output.ref_dir}/$basename/$basename.blast.out"
            bed_file="{output.ref_dir}/$basename/$basename.bed"
            output_fasta="{output.ref_dir}/$basename/$basename.bed.fasta"

            # Create BLAST database
            makeblastdb -in "$genome" -dbtype nucl -out "$db_name" >> {log} 2>&1

            # Run BLASTn to find gene locations
            blastn -db "$db_name" -query {params.query} \
                -outfmt 6 -max_target_seqs 1 -max_hsps 1 \
                -qcov_hsp_perc 90 -out "$blastn_out" >> {log} 2>&1

            # Convert BLAST output to BED format
            grep -v '^#' "$blastn_out" | \
            awk 'BEGIN{{OFS="\\t"}} {{
                if($9<=$10) print $2,$9-1,$10,$1,"0","+";
                else print $2,$10-1,$9,$1,"0","-"
            }}' | sort > "$bed_file"

            # Extract sequences using BED coordinates
            bedtools getfasta -fi "$genome" -bed "$bed_file" -name -s 2>> {log} | \
                sed "s/::.*/__$basename/g" > "$output_fasta" 2>> {log}

            # Clean up intermediate files
            rm -f "$db_name".n* "$blastn_out" "$bed_file"

            echo "Completed: $basename" >> {log}
        done

        # Clean up auxiliary index files
        find {config[references][xf_genomes]} -type f -name "*.fai" -delete 2>/dev/null || true
        find $(dirname {config[references][probes]}) -type f -name "*.fai" -delete 2>/dev/null || true

        echo "All genomes processed successfully" >> {log}
        """


rule prepare_alignment_files:
    """
    Prepare per-gene FASTA files for multiple sequence alignment.

    For each successfully reconstructed gene, combines:
      - Sequences from all reference genomes
      - Reconstructed consensus sequence from the sample
    """
    input:
        consensus_filtered = "03.probes_reconstruction/consensus/{sample}_cns_filtered.fasta",
        ref_dir = "05.phylogenetic_trees/refs/",
        xf_refs = expand("05.phylogenetic_trees/refs/{fasta}/{fasta}.bed.fasta", fasta=XF_GENOMES),
        checkpoint = "03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt"
    output:
        gene_ids = "05.phylogenetic_trees/{sample}/genes_ids.txt",
        gene_outdir = temp(directory("05.phylogenetic_trees/{sample}/genes"))
    conda:
        "envs/mapping.yaml"
    log:
        "logs/prepare_alignment_{sample}.log"
    message:
        "--- [PHYLO] Preparing alignment files for {wildcards.sample} ---"
    shell:
        """
        mkdir -p {output.gene_outdir}

        # Get list of successfully reconstructed genes and exclude plasmids
        grep "^>" {input.consensus_filtered} | grep -v "^>plasmid__" | sed 's/^>//' > {output.gene_ids}

        # Process each gene
        for gene_id in $(cat {output.gene_ids}); do
            echo "Processing gene: ${{gene_id}}" >> {log}

            # Extract reference sequences for this gene
            seqkit grep -n -r -p "^${{gene_id}}__" 05.phylogenetic_trees/refs/*/*.fasta \
                > {output.gene_outdir}/refs_${{gene_id}}.fasta

            # Extract consensus sequence for this gene
            seqkit grep -n -r -p "^${{gene_id}}$" {input.consensus_filtered} \
                > {output.gene_outdir}/cns_${{gene_id}}.fasta

            # Standardize headers
            perl -i.bak -pe 's/^>.*__/>/g' {output.gene_outdir}/refs_${{gene_id}}.fasta && rm {output.gene_outdir}/refs_${{gene_id}}.fasta.bak
            # perl -i.bak -pe 's/^>.*/>sample_{wildcards.sample}/g' {output.gene_outdir}/cns_${{gene_id}}.fasta && rm {output.gene_outdir}/cns_${{gene_id}}.fasta.bak
            perl -i.bak -pe 's/^>.*/>{wildcards.sample}/g' {output.gene_outdir}/cns_${{gene_id}}.fasta && rm {output.gene_outdir}/cns_${{gene_id}}.fasta.bak

            # Combine into single file
            cat {output.gene_outdir}/refs_${{gene_id}}.fasta \
                {output.gene_outdir}/cns_${{gene_id}}.fasta \
                > {output.gene_outdir}/${{gene_id}}.fasta

            # Clean up temporary files
            rm -f {output.gene_outdir}/refs_${{gene_id}}.fasta {output.gene_outdir}/cns_${{gene_id}}.fasta
        done
        """


rule align_and_trim_genes:
    """
    Align and trim gene sequences using MAFFT and ClipKit.

    Performs multiple sequence alignment for each gene and removes
    poorly aligned regions to improve phylogenetic inference.
    """
    input:
        gene_ids   = "05.phylogenetic_trees/{sample}/genes_ids.txt",
        genes_dir  = "05.phylogenetic_trees/{sample}/genes",
        checkpoint = "03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt"
    output:
        summary   = "05.phylogenetic_trees/{sample}/alignment_summary.txt",
        align_dir = directory("05.phylogenetic_trees/{sample}/alignments")
    resources:
        threads = 4,
        alignment_jobs = 1
    conda:
        "envs/phylogeny.yaml"
    log:
        "logs/align_genes_{sample}.log"
    message:
        "--- [PHYLO] Aligning genes for {wildcards.sample} ---"
    shell:
        """
        mkdir -p {output.align_dir}

        # Initialize summary file
        echo "Alignment and trimming summary for sample {wildcards.sample}" > {output.summary}
        echo "Analysis started: $(date)" >> {output.summary}
        echo "" >> {output.summary}

        # Process each gene
        for gene_id in $(cat {input.gene_ids}); do
            [ -z "$gene_id" ] && continue

            echo "Processing gene: $gene_id" >> {log}
            echo "Processing gene: $gene_id" >> {output.summary}

            gene_file="{input.genes_dir}/${{gene_id}}.fasta"
            aligned_file="{output.align_dir}/${{gene_id}}_aligned.fasta"
            trimmed_file="{output.align_dir}/${{gene_id}}_alignment_clipkitted.fasta"

            if [ ! -f "$gene_file" ]; then
                echo "  ERROR: Gene file not found" >> {log}
                echo "  - Status: FAILED (file not found)" >> {output.summary}
                continue
            fi

            # Run MAFFT alignment
            mafft --auto --thread {resources.threads} "$gene_file" > "$aligned_file" 2>> {log}

            # Trim alignment with ClipKit
            clipkit "$aligned_file" -o "$trimmed_file" >> {log} 2>&1
            rm -f "$aligned_file"

            if [ -s "$trimmed_file" ]; then
                echo "  - Status: SUCCESS" >> {output.summary}
            else
                echo "  - Status: FAILED" >> {output.summary}
            fi
        done

        echo "" >> {output.summary}
        echo "Analysis completed: $(date)" >> {output.summary}
        """


rule phylogenetic_analysis:
    """
    Construct phylogenetic tree using IQ-TREE.

    Concatenates aligned genes and performs partitioned phylogenetic analysis
    with ModelFinder for optimal model selection and bootstrap support.
    """
    input:
        gene_ids = "05.phylogenetic_trees/{sample}/genes_ids.txt",
        alignment_summary = "05.phylogenetic_trees/{sample}/alignment_summary.txt",
        checkpoint = "03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt"
    output:
        concat_alignment = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}_concatenated_alignment.phy",
        partition_file = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}_partitions.txt",
        iqtree_log = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}.log",
        iqtree_tree = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}.contree"
    params:
        model = "MFP+MERGE",
        tree_dir = "05.phylogenetic_trees/{sample}/consensus_tree",
        align_dir = "05.phylogenetic_trees/{sample}/alignments"
    resources:
        threads = config['threads'].get('iqtree', 8),
        iqtree_jobs = 1
    conda:
        "envs/phylogeny.yaml"
    log:
        "logs/phylogeny_{sample}.log"
    message:
        "--- [PHYLO] Building phylogenetic tree for {wildcards.sample} ---"
    shell:
        """
        mkdir -p {params.tree_dir}

        # Concatenate alignments using AMAS
        AMAS.py concat \
            --in-files {params.align_dir}/*_alignment_clipkitted.fasta \
            --in-format fasta \
            --data-type dna \
            --concat-out {output.concat_alignment} \
            --out-format phylip \
            --concat-part {output.partition_file} \
            --part-format nexus \
            >> {log} 2>&1

        # Run IQ-TREE with partitioned analysis
        iqtree \
            -s {output.concat_alignment} \
            -p {output.partition_file} \
            -T {resources.threads} \
            -pre {params.tree_dir}/{wildcards.sample} \
            -m {params.model} \
            -B 1000 \
            --quiet \
            >> {log} 2>&1

        # Clean up auxiliary files
        find {params.tree_dir} -type f ! \
            \\( -name "*.contree" -o -name "*.log" -o -name "*_concatenated_alignment.phy" -o -name "*_partitions.txt" \\) \
            -delete 2>/dev/null || true
        """


rule plot_tree:
    """
    Generate phylogenetic tree visualization.

    Creates a PNG image of the phylogenetic tree using R/ggtree.
    """
    input:
        iqtree_tree = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}.contree"
    output:
        tree_png = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}_tree.png"
    params:
        plot_script = str(UTILS_DIR / "plot_tree.R"),
        plot_prefix = "05.phylogenetic_trees/{sample}/consensus_tree/{sample}",
        sample_name = "{sample}"
    conda:
        "envs/r_tools.yaml"
    log:
        "logs/plot_tree_{sample}.log"
    message:
        "--- [PHYLO] Plotting phylogenetic tree for {wildcards.sample} ---"
    shell:
        """
        unset R_HOME
        export R_LIBS_USER=""
        export R_LIBS_SITE=""
        Rscript \
            {params.plot_script} \
            {input.iqtree_tree} \
            {params.plot_prefix} \
            {params.sample_name} \
            >> {log} 2>&1
        """


rule phylogeny_phase:
    """
    Phase 2 target rule: Phylogenetic analysis for successful samples.

    Orchestrates the phylogenetic analysis workflow only for samples
    that passed the reconstruction checkpoint.
    """
    input:
        # Ensure Phase 1 is complete
        mlst = "04.mlst-typing/mlst_summary.csv",
        reconstruction_check = expand("03.probes_reconstruction/checkpoint/{sample}_reconstruction_check.txt", sample=SAMPLES),

        # Reference genome extractions
        xf_ref_dir = "05.phylogenetic_trees/refs/",
        xf_ref_fasta = expand("05.phylogenetic_trees/refs/{fasta}/{fasta}.bed.fasta", fasta=XF_GENOMES),

        # Phylogenetic outputs (dynamically expanded for successful samples only)
        gene_ids = lambda wildcards: expand("05.phylogenetic_trees/{sample}/genes_ids.txt",
                                            sample=get_successful_samples()),
        alignment_summary = lambda wildcards: expand("05.phylogenetic_trees/{sample}/alignment_summary.txt",
                                                     sample=get_successful_samples()),
        iqtree_tree = lambda wildcards: expand("05.phylogenetic_trees/{sample}/consensus_tree/{sample}.contree",
                                               sample=get_successful_samples()),
        tree_plots = lambda wildcards: expand("05.phylogenetic_trees/{sample}/consensus_tree/{sample}_tree.png",
                                              sample=get_successful_samples())
    output:
        completion = "05.phylogenetic_trees/phylogeny_analysis_complete.txt"
    log:
        "logs/phylogeny_phase.log"
    message:
        "--- [PHYLO] Phase 2: Phylogenetic analysis complete ---"
    shell:
        """
        echo "Phylogenetic Analysis Summary" > {output.completion}
        echo "============================" >> {output.completion}
        echo "Analysis started: $(date)" >> {output.completion}
        echo "" >> {output.completion}

        # Count processed samples
        total_samples=$(ls 03.probes_reconstruction/checkpoint/*_reconstruction_check.txt 2>/dev/null | wc -l)
        successful_count=0
        failed_count=0

        echo "Processing checkpoint results:" >> {output.completion}

        for checkpoint_file in 03.probes_reconstruction/checkpoint/*_reconstruction_check.txt; do
            [ -e "$checkpoint_file" ] || continue
            sample=$(basename "$checkpoint_file" _reconstruction_check.txt)

            if grep -q "SUCCESS:" "$checkpoint_file"; then
                genes_count=0
                partition_file="05.phylogenetic_trees/$sample/consensus_tree/${{sample}}_partitions.txt"

                if [ -f "$partition_file" ]; then
                    genes_count=$(grep -c "charset" "$partition_file" 2>/dev/null || echo 0)
                fi

                echo "✓ Sample $sample: phylogenetic analysis completed ($genes_count genes used)" >> {output.completion}
                successful_count=$((successful_count + 1))
            else
                echo "✗ Sample $sample: excluded from phylogenetic analysis" >> {output.completion}
                failed_count=$((failed_count + 1))
            fi
        done

        echo "" >> {output.completion}
        echo "Summary:" >> {output.completion}
        echo "- Total samples processed: $total_samples" >> {output.completion}
        echo "- Successful samples (phylogeny): $successful_count" >> {output.completion}
        echo "- Failed samples (excluded): $failed_count" >> {output.completion}
        echo "" >> {output.completion}
        echo "Analysis completed: $(date)" >> {output.completion}

        if [ $successful_count -eq 0 ]; then
            echo "WARNING: No samples were successful for phylogenetic analysis!" >> {output.completion}
            echo "WARNING: No samples were successful for phylogenetic analysis!" >> {log}
        else
            echo "SUCCESS: Phylogenetic analysis completed for $successful_count samples" >> {log}
        fi
        """
